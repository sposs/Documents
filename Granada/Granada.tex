\documentclass{beamer}
%\usepackage{beamerarticle}
\usepackage{heppennames}
\usepackage{hepnicenames}
\usepackage{graphicx} 
\usepackage{multirow}
\usepackage{amsbsy,amsmath,amssymb}
\usepackage{booktabs}
% ********** Styl prezentacji **********
\mode<presentation>
{
	\usetheme{Singapore}
  \setbeamercovered{transparent}
   \setbeamertemplate{footline}[frame number] 
  \setbeamertemplate{navigation symbols}{ 
  \insertslidenavigationsymbol
  \insertframenavigationsymbol
  \insertsubsectionnavigationsymbol
  \insertsectionnavigationsymbol
  \insertdocnavigationsymbol
  \insertbackfindforwardnavigationsymbol
  \hskip 0.3cm
  %\insertframenumber / \inserttotalframenumber  % <<< frame #
  %\insertpagenumber / \insertpresentationendpage % <<< page #
} 
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

% font definitions, try \usepackage{ae} instead of the following
% three lines if you don't like this look
\usepackage{mathptmx}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}


\usepackage[T1]{fontenc}
\author{S. Poss, C.B. Lam, J. Strube, C. Grefe}
\institute[CERN]{CERN}

\subject{CLICProduction}
\AtBeginSubsection[]
{
	\begin{frame}<beamer>
		\frametitle{Outline}
		\tableofcontents[currentsection,currentsubsection]
	\end{frame}
}

\title[]{Monte Carlo production for the CLIC CDR}
\subtitle{Our experience}

\date{\today}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}
	\frametitle{Table of contents}
	\begin{itemize}
\item Introduction
\item Production framework
\item CLIC productions
\item Other considerations
\item Conclusion
\end{itemize}
\end{frame}

\section{Introduction}

\begin{frame}
	\frametitle{Introduction}
\begin{itemize}
\item 6 benchmark analysis: 
\begin{itemize}
\item $\Pep\Pem \to \Ph \Pgne \Pagne$,
\item  $\Pep\Pem \to \PHp \PHm$, $\Pep\Pem \to \PHz \PA$, 
\item $\Pep\Pem \to \PSq_R \PaSq_R$, 
\item $\Pep\Pem \to \PSl \PaSl \,(\ell = \Pe,\Pgm)$, 
\item $\Pep\Pem \to \PSgxpm_i \PSgxmp_j,\, \Pep\Pem \to \PSgxz_i \PSgxz_j$,
\item  $\Pep\Pem \to \Pqt \Paqt$ (500~GeV).
\end{itemize}
\item Plus all the backgrounds
\end{itemize}
Total number of events generated in the last 6 months: \alert{$17\cdot 10^6$}, events processed: $\sim 100\cdot 10^6$.
\end{frame}

\part{Framework}
\begin{frame}
\partpage 
\end{frame}

\section{Production framework}
\begin{frame}
	\frametitle{Production framework}
DIRAC has a Transformation System:
\begin{itemize}
\item  Create a workflow object (XML representation of a job), and let the system create the jobs for you
\item Automatic resubmission of failed jobs: minor monitoring needed
\end{itemize}
~\\
It \alert{saved us a huge amount of time and man power}.
\end{frame}

\begin{frame}
\frametitle{Production framework}
The DIRAC transformation system comes with a useful set of API commands: get properties, change them, update inputs, etc.\\
~\\
\alert{Full support for the ILD and SiD software frameworks}, plus support for WHIZARD and PYTHIA code. Additionnaly we have support for generator level cuts (StdhepCut tool from Lars Weuste), Overlay input, etc.~\\

\end{frame}

\begin{frame}
\frametitle{Defining steps}
\begin{itemize}
\item Due to uncertain CPU usage, it was decided to split all steps in different jobs.
\item Implies dedicated productions for:
\begin{itemize}
\item Generation: WHIZARD/PYTHIA.
\item Simulation: Mokka/SLIC.
\item Reconstruction: Marlin/LCSIM-SLICPandora-LCSIM.
\end{itemize}
\item Limitation: few number of events per step ($10-50$), so small files to carry around.
\item Linking of productions is straightforward, more on that later: \alert{data driven production management}.
\end{itemize}
All files produced are stored in the DIRAC replica and Metadata catalog
\end{frame}

\section{Replica Catalog and Metadata catalog}
\begin{frame}
\frametitle{Replica catalog} 
\begin{itemize}
\item Use of LFC is somewhat \alert{limited in performance sometimes} (more true for the LHC though)
\item \alert{DIRAC provides a catalog} to overcome this
\item Has also the possibility to \alert{write and read in the LFC via the same interface}.
\end{itemize}
In the futur (coming month), when adding a file in the DIRAC FC, it will also end up in the ILC LFC. It would be nice that the files be always added in both catalogs.
 \end{frame}
\begin{frame}
\frametitle{Metadata catalog}
\begin{itemize}
\item ILC stores the metadata encoded in the file names.
\item Somehow restricts the number of meta info that one can add to a given file once the file is created.
\item DIRAC FC comes with a metadata catalog, that sets the \alert{metadata at directory level} (e.g. production ID, software packages, cross section, polarisation, etc.) and at File level (number of events, etc.).
\item Flexible as one can add (and remove) metadata on the fly.
\item Searchable: find EvtType=tt Datatype=SIM
\item Possibility to set ACLs.
\item Also comes with convenient API commands.
\end{itemize} 
Web interface is under development, should be available for tests in the coming weeks.
 \end{frame}
\section{Monitoring}
\begin{frame}
\frametitle{Production Monitoring}
Monitoring done with web site: 
\begin{itemize}
\item Overview of production statuses: \% of the files processed,\\ failure rate
\item Access to jobs from a given production in one click
\item Monitoring and accounting plots also available
\end{itemize}
\end{frame}

\part{CLIC Production}
\begin{frame}
\partpage 
\end{frame}

\section{Production}
\begin{frame}
\frametitle{Defining a production} 
\begin{enumerate}
\item Define the input data (if any) via a metadata query (e.g. meta['ProdID']=186)
\item Define the application and versions you want to use
\item Define the output files (if any)
\end{enumerate}
\alert{Data driven procedure}: if new files are added to the catalog that correspond to the input data query, jobs are automatically created.
\end{frame}


\section{Generation}
\begin{frame}
\frametitle{Generating Events}
Use WHIZARD for most samples:
\begin{itemize}
\item Handling of \alert{beam spectrum and ISR included}
\item Adding a \alert{new process} is straight forward
\item Does the correct computation of diagrams and interference: \alert{accurate cross section and kinematics estimation}
\item Limitation: \alert{final states have no width}, cannot do $\Pqt\Paqt$, $\PWp\PWm$ or $\PZz\PZz$.
\end{itemize} 
Use PYTHIA for those 3 samples:
\begin{itemize}
\item Final states have the correct width
\item Not so accurate estimation of cross section or kinematics, but simple enough to be very similar to those produced by WHIZARD
\item Has \alert{beam spectrum interface via CALYPSO}, ISR is PYTHIA default
\item Limitation: \alert{no flexibility at all}, adding a channel cannot be done easily
\end{itemize}
\end{frame}

\section{Simulation}
\begin{frame}
\frametitle{Simulation}
Define productions for both detector concepts, but very similar interface.  
\end{frame}

\section{Reconstruction with Overlay}
\begin{frame}
\frametitle{Reconstruction}
 As for the simulation: dedicated productions are created, automatically using files produced by the simulation step.\\
~\\
Reconstruction done with and without Overlay.
\end{frame}
\begin{frame}
\frametitle{Overlay handling (1)}
CLIC detector benchmark require to reconstruct the events with overlaid $\gamma\gamma \to had$. \alert{60 BX with 3.2 interactions per bunch Xings are considered for every signal event}.\\
~\\
Simulated background files contain $100$ events each (more would be better but CPU is a constrain). For a 10 signal events file, one need $\sim 2000$ bkg events, or \alert{$20$ files per job}.
\end{frame}
\begin{frame}
\frametitle{Overlay handling (2)}
This did not work: storages (CERN in particular, but also IN2P3) do not cope with such a load (2000 jobs trying to access 20 random files each represent a huge number of queries).\\
Solution: \alert{merge the files randomly once} to reduce the number of files needed: merge them by packs of 200.\\ 
~\\
Works but a bit tedious: requires to have all the files locally once. Plus one needs a \alert{very large number of files} to have enough combinaisons.\\
~\\
Needed solution: \alert{use random access} (LCIO v1.51 has it, but not LCSIM). Then only one big file is needed on every site with direct access. No more transfers are needed.
\end{frame}

\section{Data validation}
\begin{frame}
\frametitle{Data validation}
When simulating data with 2 frameworks, it's necessary to validate it before running. For that purpose \alert{TOMATO} was designed: convert the stdhep to slcio (stdhepjob supported by DIRAC), and run TOMATO (Marlin processor) to create histograms of "significant" distributions.  \\
~\\
"Significant" is specific to the different analysis, so every working group needs to provide the variables to be plotted.\\
~\\
Was used for the $\Ptop\APtop$ analysis.
\end{frame}

\part{Prospects}
\begin{frame}
\partpage 
\end{frame}

\begin{frame}
\frametitle{Prospects} 
Testing running in the US sites: when it runs, we'll need to see for disk space here and there. CERN disk requests will be made: for the moment, only 40Tb are available on disk (as mush as needed on TAPE though).\\
~\\
Will finish implementing dedicated service that will know the available processes, to easy users' lives. For the moment, a text file holds all the processes available and the corresponding WHIZARD version.\\
~\\
Will push to get the File Catalog web browser.
\end{frame}

\part{Conclusion}
\begin{frame}
\partpage 
\end{frame}

\begin{frame}
\frametitle{Conclusion} 
\begin{itemize}
\item Produced in 6 month $\approx 100$ million events in 9 months
\item Used $2\cdot 10^{10}$ KSI2K seconds of CPU in $\sim 20$ sites
\item Data produced correspond to $\sim 130$~TB
\item Most problems were due to storage access, in particular for the overlay files
\item ILC DBD could benefit from DIRAC usage
\end{itemize}
At least, the data produced should be stored in both catalogs.
\end{frame}

\appendix
\part{Backup slides}
\begin{frame}
\partpage 
\end{frame}
\section{Resources used}
\begin{frame}
\frametitle{Resources used: CPU}
 \includegraphics[width=10cm]{CPUperSiteForProd}
\end{frame}
\begin{frame}
\frametitle{Resources used: Storage} 
\begin{center}
\begin{tabular}{crr}
\toprule
Site & Production (TB) & User (TB) \\
\midrule
CERN & 128 & 20\\
IN2P3 & 4 & 9\\
RAL & 4 & 28\\
KEK & 0.02 & 0\\
IMPERIAL& 1.6 & 0 \\
TAU & $4\cdot 10^{-4}$ & 0\\
\bottomrule
\end{tabular}
\end{center}
Sites that can be used in addition: DESY, Bristol, BONN, RALPP
\end{frame}
\section{Technical aspects}
\begin{frame}
\frametitle{Technical aspects}
Dropped pilot mode as some sites did not support it, using private pilot mode\\
~\\
Steering files are installed on the sites like applications (dependency relation), so no need to pass them in the input sandbox of the job.\\
~\\
Software installed in the Shared areas of the sites when/where possible. Does not use SAM framework (yet) as DIRAC takes care of the software installation and removal. 
\end{frame}

\begin{frame}
\frametitle{Services availability}
100 million events correspond to $\sim 4$ million jobs. On average 1000 concurrently running.\\
~\\
File catalog is the most expensive service in terms of CPU and simultaneous queries to DB. Had to duplicate service (not DB). ~\\
~\\
It would be useful to replicate the DB and other services to have more instances available (more VO boxes needed).~\\
~\\
Same problem with JobManager with high load.
\end{frame}

\section{Conventions}
\begin{frame}
\frametitle{File names conventions}
\begin{itemize}
\item Base path is /ilc/prod to separate from /ilc/user data. 
\item Path holds most relevant info (machine, energy, process, detector, data type (gen, SIM, REC, DST), prodID)
\item File name is process\_prodID\_jobID.ext (ext=stdhep,slcio)
\end{itemize} 
 \end{frame}

\end{document}
